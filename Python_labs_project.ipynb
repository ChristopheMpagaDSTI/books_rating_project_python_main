{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2729131c",
   "metadata": {
    "tags": [
     "Project_plan"
    ]
   },
   "source": [
    "Project plan\n",
    "\n",
    "1. problem formulation :  \n",
    "Context : we have been instructed to predict books ratings via a machine learning application from provided data.  \n",
    "2. Data preprocessing\n",
    "3. EDA\n",
    "4. features engineering \n",
    "5. modelling & Machine learning \n",
    "6. Model validation\n",
    "7. deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b1fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ab7ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# activate environment \n",
    "#!conda init bash\n",
    "#!conda activate Project1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c92fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c5c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import seaborn as sns\n",
    "#import matplotlib as plt\n",
    "#import plotly as ply\n",
    "import sklearn as skl\n",
    "import datetime as dt\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "import re \n",
    "from IPython.display import clear_output, display, HTML, Javascript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0058b45",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# load data \n",
    "df = pd.read_csv(\"books.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0df230",
   "metadata": {},
   "source": [
    "2 Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da0d13",
   "metadata": {},
   "source": [
    "2.1  inspect data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54080887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cd1ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3eb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df shape \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f74529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns names\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91609d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe numeric variables\n",
    "df.describe(exclude=\"object\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0043404",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# describe non-numeric  variables\n",
    "df.describe(include=\"object\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38cba68",
   "metadata": {},
   "source": [
    "There is no duplicate ID in data as isbn is 11127 the datat set number of cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5128da2a",
   "metadata": {},
   "source": [
    "2.2  clean data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c261187e",
   "metadata": {},
   "source": [
    "2.2.1 Clean publication date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58c05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change from string to datetime object \n",
    "# for loop to parse date \n",
    "df[\"publication_Date\"]= pd.Series(dtype=\"int\")\n",
    "cnt = 0\n",
    "for i in range(len(df.publication_date)) :\n",
    "    month,day,year = map(int,df.publication_date[i].split('/'))\n",
    "\n",
    "    if month in [4,6,9,11] and day > 30:\n",
    "        day = 30\n",
    "        df[\"publication_Date\"][i] = dt.date(year,month,day)\n",
    "    elif(month==2 and day > 28 and year % 4 != 0):\n",
    "        day = 28\n",
    "    df[\"publication_Date\"][i] = dt.date(year,month,day)\n",
    "    cnt +=1\n",
    "if (cnt == len(df.publication_date)) :\n",
    "    print(\"no date issue\" )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c763a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove heading whitespace from num_pages feature \n",
    "df = df.rename(columns = {\"  num_pages\" : \"num_pages\"})\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb646068",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#na check in the entire dataset \n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d17ae46",
   "metadata": {},
   "source": [
    "There is no NA in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd8019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# na check in average_rating\n",
    "if (df.average_rating.isna().mean() == 0 ) :\n",
    "    print(\"There is no NA in average_rating\")\n",
    "else:\n",
    "    print(\"There are NA in average_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f808cc77",
   "metadata": {},
   "source": [
    "3 EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbf210f",
   "metadata": {},
   "source": [
    "1D EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b0bcb",
   "metadata": {},
   "source": [
    "3.1 Numerical summary and 1D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c9e4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical summary : average_rating \n",
    "df.average_rating.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b9a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of average ratings \n",
    "df.average_rating.hist(bins=60,legend={\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fe9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of average ratings :range [0,3.9]\n",
    "df.average_rating.hist(bins=60,range = [0,3],legend={\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a247bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of average ratings :range [0,4.75]\n",
    "df.average_rating.hist(bins=30,range = [3,4.75],legend={\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8159b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of average ratings :range [0,3.9]\n",
    "df.average_rating.hist(bins=30,range = [4.75,5],legend={\"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73733d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of cases where averating are <= 3\n",
    "df.loc[df.average_rating <= 3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8da9b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of cases where average rating >= 4.75 \n",
    "df.loc[df.average_rating > 4.75].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a453e8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average ratings sumary statistics \n",
    "df.average_rating.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc497d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#average_rating 1st and 3rd quantile \n",
    "IQR_average_rating = df.quantile([0.25,0.75])[[\"average_rating\"]]\n",
    "IQR_average_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76473b7b",
   "metadata": {},
   "source": [
    "average_ratings is almost an unimodal distribution with some outliers.\n",
    "it has median 3.96 and IQR : 0.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#numerical summary num_pages\n",
    "df.num_pages.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c9e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of num_pages \n",
    "df.num_pages.hist(bins = 60,legend={\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aef50c",
   "metadata": {},
   "source": [
    "Num_pages is almost an unimodal distribution, right skewed showing that  at least 50% of books presents 299 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical summary ratings_count \n",
    "(df.ratings_count\n",
    ".quantile([0.25,0.5,0.75, 1])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc77d5b2",
   "metadata": {},
   "source": [
    "Maximum rating count is so high that it seems suspect !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49a1477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which book has this ratings count ?\n",
    "df.title[df.ratings_count == max(df.ratings_count) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a677b8",
   "metadata": {},
   "source": [
    "Twilight is the book with the highest ratings_count.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9129cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# histogram of ratings_count \n",
    "df.ratings_count.hist(bins = 50,range = [0,2000],legend ={\"\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad8750e",
   "metadata": {},
   "source": [
    "Ratings count distribution is unimodal, right skewed. At least 50% of books ratings count is greater or equal to 745."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1084fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical summary publication Date\n",
    "df.publication_Date.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1105419",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    "[\"publication_Date\"]\n",
    ".quantile([0.0,0.25,0.5,0.75,1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c09838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of publication date \n",
    "df.publication_Date.hist(bins = 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49a73e",
   "metadata": {},
   "source": [
    "publication date distribution is unimodal and left skewed. At least 50% of books has been published before between 1998 and 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76fb1ad",
   "metadata": {},
   "source": [
    "How many books are written in english ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language code \n",
    "df3 = df.groupby(\"language_code\").count()\n",
    "# round \n",
    "round(df3.bookID.filter(regex =\"^[Ee][Nn][g\\\\-]\")/df.shape[0],2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b5734",
   "metadata": {},
   "source": [
    "95% of book are written in English. The english type being diffrent.  \n",
    "It may be reasonble to turn language code into a binary variable.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a9b1b5",
   "metadata": {},
   "source": [
    "Who are the five most prolific authors ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f80fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# authors\n",
    "(df.groupby(\"authors\")\n",
    " .count()\n",
    " .sort_values(\"title\",ascending=False)\n",
    " [[\"title\"]]\n",
    " .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ce2b7",
   "metadata": {},
   "source": [
    "5 most prolific authors are :  \n",
    "1. S. king\n",
    "2. P.G. Wodehouse\n",
    "3. Rumiko Takahashi\n",
    "4. Orson Scott Card\n",
    "5. Agatha Christie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104cb125",
   "metadata": {},
   "source": [
    "Who are the 5 more prolific publisher ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eda2031",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby(\"publisher\")\n",
    " .count()\n",
    " .sort_values(\"title\",ascending=False)\n",
    " [[\"title\"]]\n",
    " .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37612350",
   "metadata": {},
   "source": [
    "The 5 most productive publisher are :   \n",
    "1. Vintage\n",
    "2. Penguin Books \n",
    "3. Penguin Classics\n",
    "4. Mariner Books\n",
    "5. Ballantine Books"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38750251",
   "metadata": {},
   "source": [
    "What are the first 5 well rated titles ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ec4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby(\"title\")\n",
    " .agg(\"mean\")\n",
    " .sort_values(by =\"average_rating\",ascending=False)\n",
    " [[\"average_rating\"]]\n",
    " .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fab72b9",
   "metadata": {},
   "source": [
    "Above are the 5 well rated books title in this dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac548d06",
   "metadata": {},
   "source": [
    "We are not going to do EDA on bookID, isbn , isbn13. \n",
    "The first 3 are identifier and have no variation, though it won't bring enough information to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c84ae8",
   "metadata": {},
   "source": [
    "Turn language code into a binary variable and perform EDA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fa32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin language code  eng : 1 , other : 0\n",
    "language_code_bin = []\n",
    "for i in range(len(df.language_code)) :\n",
    "    if re.match(\"^([Ee][Nn].+)\",df.language_code[i]):\n",
    "        language_code_bin.append(1)\n",
    "    else:\n",
    "        language_code_bin.append(0) \n",
    "df[\"language_code_bin\"]  = language_code_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995db1ae",
   "metadata": {},
   "source": [
    "What is the proportion of books written in English language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f9b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of english in language code. \n",
    "(df.groupby(\"language_code_bin\")\n",
    " .count()/len(df.language_code_bin)\n",
    " ).round(3)[[\"title\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9365265",
   "metadata": {},
   "source": [
    "English language has a proportion of 0.95 in language code. 95% of our books are written in English. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb7ed4",
   "metadata": {},
   "source": [
    "== Finding association between variables ==  \n",
    " 3.2 Numerical summary and  2D plot \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b37b0b1",
   "metadata": {},
   "source": [
    "Is there any association between number of pages and average rating ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73705dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot average rating vs num_pages\n",
    "df.plot.scatter(x=\"num_pages\",y = \"average_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814c2a4b",
   "metadata": {
    "tags": [
     "correlation_coef"
    ]
   },
   "outputs": [],
   "source": [
    "# correlation cofficient between average rating and num_pages\n",
    "round(df.average_rating.corr(df.num_pages),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb555e6",
   "metadata": {
    "tags": [
     "summary"
    ]
   },
   "source": [
    "The correlation coeffication is 0.15. This suggest a weak association between \n",
    "average rating and number of pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0324b724",
   "metadata": {
    "tags": [
     "2D_plot"
    ]
   },
   "outputs": [],
   "source": [
    "# scatter plot average_rating vs text_reviews_count \n",
    "df.plot.scatter(x=\"text_reviews_count\",y = \"average_rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4e9669",
   "metadata": {},
   "source": [
    "There is not a clear pattern between these 2 variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dd7701",
   "metadata": {
    "tags": [
     "corr_coef"
    ]
   },
   "outputs": [],
   "source": [
    "# correlation coef\n",
    "round(df.average_rating.corr(df.text_reviews_count),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9478d004",
   "metadata": {},
   "source": [
    "The correlation coef is almost 0. This suggest no association between average rating and text reviews count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c1d079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatter plot average rating. vs ratings count \n",
    "df.plot.scatter(x=\"ratings_count\",y = \"average_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b2b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation coeff between average rating and ratings count \n",
    "round(df.average_rating.corr(df.ratings_count),2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24256a43",
   "metadata": {},
   "source": [
    "The correlation coefficient between average rating and ratings count is 0.04. This suggest that there is almost no association between average rating and rating count. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d91f8d",
   "metadata": {},
   "source": [
    "what is the average rating of books written in english ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfb0ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df.groupby(\"language_code_bin\")\n",
    " .agg(\"mean\")\n",
    "[[\"average_rating\"]]\n",
    ".round(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580bde4",
   "metadata": {},
   "source": [
    "On average,books written in English are less well rated than books written in other languages.  but the the difference near 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932af2d",
   "metadata": {},
   "source": [
    "=== 2D EDA SUMMARY ===    \n",
    "-numerical features are not strongly associated with average rating if considering a linear assosiation. In a linear modeling numerical features have no effect or a little one on average rating. Let's investigate other models i.e. tree based model or svm. \n",
    "\n",
    "Qualitative variable,i.e. language_code has on average a rating of 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7476ebae",
   "metadata": {},
   "source": [
    "4 === features engineering part1  ===  \n",
    "we are going to remove bookID, title , authors,isbn,isbn13, publisher because they have or present no variation and are less informative for our application.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c860b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe : df1 and remove some features \n",
    "df_qualitative = df[['bookID','title','authors','isbn','isbn13',\n",
    "               'publisher']]\n",
    "df_qualitative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qualitative.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da42a77",
   "metadata": {},
   "source": [
    "4.1 Discretize average_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edc2361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretize average rating \n",
    "av_cat = []\n",
    "av_cat = pd.cut(df.average_rating,bins=[0,1,2,3,4,5,6],\n",
    "                    labels = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\"],\n",
    "                    include_lowest=True)\n",
    "df_qualitative[\"av_cat\"] = av_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693c6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# head average rating \n",
    "df_qualitative.av_cat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26460dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail av rating \n",
    "df_qualitative.av_cat.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed26b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of classes in the data \n",
    "round(df_qualitative.av_cat.value_counts()/ len(df_qualitative.av_cat),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ce0e44",
   "metadata": {},
   "source": [
    "There is 40% of data in class 4 and 56% in class 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d582e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qualitative.av_cat.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cf208c",
   "metadata": {},
   "source": [
    "Discretizing average_rating  make it unbalanced data.  \n",
    "We need to fix unbalalced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4da1fd",
   "metadata": {},
   "source": [
    "================================ REGRESSION ========================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdbee6",
   "metadata": {},
   "source": [
    "4.2 Splitting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be1e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting numerical features\n",
    "df_final = df[['average_rating', 'num_pages','text_reviews_count', \n",
    "          \"language_code_bin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train0,test = train_test_split(df_final, test_size= 20,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8467d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train0 in train and valid \n",
    "train,valid = train_test_split(train0,test_size=40,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52ab29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting \n",
    "# train0\n",
    "X_train0 = train0.drop(\"average_rating\",axis = 1)\n",
    "y_train0 = train0.average_rating\n",
    "\n",
    "# train\n",
    "X_train = train.drop(\"average_rating\",axis = 1)\n",
    "y_train = train.average_rating\n",
    "\n",
    "# valid \n",
    "X_valid = valid.drop(\"average_rating\",axis=1)\n",
    "y_valid = valid.average_rating\n",
    "\n",
    "# test \n",
    "X_test = test.drop(\"average_rating\",axis = 1)\n",
    "y_test = test.average_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b32b14",
   "metadata": {},
   "source": [
    "5 modelling : model testing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a64e83",
   "metadata": {},
   "source": [
    "5.1 Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753dc5c",
   "metadata": {},
   "source": [
    "-linear model : OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aefb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model \n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342c7a1",
   "metadata": {},
   "source": [
    "-regularized linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c2792b",
   "metadata": {},
   "source": [
    "-Ridge : alpha = 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e71c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularized model : ridge model, alpha = 0.1 \n",
    "reg_ridge = linear_model.Ridge(alpha=0.01)\n",
    "reg_ridge.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf05111",
   "metadata": {},
   "source": [
    "-ridge alphas, cv = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2dd4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ridge model with cross validation \n",
    "# regularized model : ridge model with cv =5\n",
    "reg_ridge_CV = linear_model.RidgeCV(alphas=[0.001,0.01],cv=5)\n",
    "reg_ridge_CV.fit(X_train0,y_train0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d921b419",
   "metadata": {},
   "source": [
    "-Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e11c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn import linear_model, alpha = 0.5\n",
    "reg_lasso = linear_model.Lasso(alpha=0.5)\n",
    "reg_lasso.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe850d2",
   "metadata": {},
   "source": [
    "5.2 Tree based model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c69f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree based model \n",
    "from sklearn import tree\n",
    "regressor = tree.DecisionTreeRegressor(random_state=40)\n",
    "tree_reg= regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487847d",
   "metadata": {},
   "source": [
    "5.3 Ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63624c1",
   "metadata": {},
   "outputs": [],
   "source": [
    " #random forest\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "rf = RandomForestRegressor(n_estimators= 100,\n",
    "#max_depth = 5,\n",
    "random_state=42)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8671773",
   "metadata": {},
   "source": [
    "5.4 Support vectors machine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974678b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear svm\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_features=4, random_state=0)\n",
    "svm_regr = make_pipeline(StandardScaler(),\n",
    "                     LinearSVR(random_state=0, tol=1e-5,max_iter=100000))\n",
    "svm_regr.fit(X, y)\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf098e",
   "metadata": {},
   "source": [
    "5.5 gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2602a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbm = GradientBoostingRegressor(random_state=0)\n",
    "gbm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54b8566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross val custom function \n",
    "def model_cross_val(models={}, X_validation=None,y_validation= None,cv=None, scoring = \"neg_mean_squared_error\"):\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    table = []\n",
    "    for model in models :\n",
    "        score = abs(\n",
    "            cross_val_score(models[model],\n",
    "                            X_validation,\n",
    "                            y_validation,\n",
    "                            cv=cv,\n",
    "                            scoring=scoring ).mean()\n",
    "        )\n",
    "        score = pow(score,.5)\n",
    "        table.append(score)\n",
    "    print(\n",
    "         pd.DataFrame(data = table,index = models,columns = [\"rmse\"])\n",
    "        .sort_values(by =\"rmse\")\n",
    "        \n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f395fcd2",
   "metadata": {},
   "source": [
    " Models dictionary for performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2b054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models to cross-validate \n",
    "\n",
    "# models to cross validate\n",
    "models = {\"linear reg\": reg,\n",
    "          \"ridge model\":reg_ridge,\n",
    "          \"lasso model \": reg_lasso,\n",
    "          \"ridge CV\":reg_ridge_CV,\n",
    "          \"random forest\":rf,\n",
    "          \"linear svm\":svm_regr,\n",
    "           \"gbm\": gbm\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2588bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validation  \n",
    "model_cross_val(models,X_valid,y_valid,cv= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f140ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect and remove outliers function\n",
    "def remove_outliers(X,bins = 50, plot = \"bool\"):\n",
    "    # q1 and q3 quantiles\n",
    "    q1,q3 = X.quantile([0.25,0.75])\n",
    "\n",
    "    #interquatile range \n",
    "    iqr = q3 - q1\n",
    "   \n",
    "\n",
    "    # remove  q1 - 1.5*iqr or  q3 + 1.5*iqr:\n",
    "    X =  X[X.between(q1 - 1.5*iqr, q3 + 1.5*iqr,inclusive = \"both\")]\n",
    "       \n",
    "\n",
    "    # histogram\n",
    "    if plot :\n",
    "          X.hist( bins = bins,legend = {\"\"})\n",
    "    return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830d197d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove ouliers OR NOT\n",
    "q1,q3 = df.average_rating.quantile([.25,.75])\n",
    "iqr = q3 - q1\n",
    "#df_mdl = df_final[df_final.average_rating.between(q1 - 1.5*iqr, q3 + 1.5*iqr,inclusive = \"both\")]\n",
    "df_mdl = df[[\"average_rating\",\"num_pages\",\"language_code_bin\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a694e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdl.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f3d73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mdl.average_rating.hist(bins = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remove_outliers(df_mdl.average_rating, bins = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfee356",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_outliers(df_mdl.average_rating, plot = False).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c05c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers from num_pages\n",
    "remove_outliers(df_mdl.num_pages,bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cc74b1",
   "metadata": {},
   "source": [
    "Removing outliers, concentrate average rating between 3.9 and 4.6. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579eb540",
   "metadata": {},
   "source": [
    "We end up with 11098 cases.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bac2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split data\n",
    "train,test = train_test_split(df_mdl, test_size= 0.4,random_state=42)\n",
    "\n",
    "# train\n",
    "X_train = train.drop(\"average_rating\",axis=1)\n",
    "y_train = train.average_rating\n",
    "\n",
    "# test\n",
    "X_test = test.drop(\"average_rating\",axis=1)\n",
    "y_test = test.average_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6734a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f9db08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model \n",
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats model package\n",
    "X = sm.add_constant(X_train)\n",
    "mdl = sm.OLS(y_train,X_train)\n",
    "fit = mdl.fit()\n",
    "# print model \n",
    "print(fit.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88449bdb",
   "metadata": {},
   "source": [
    "In a linear regression setting, ratings_count has a negative influence on average rating  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd933a0",
   "metadata": {},
   "source": [
    "!!! Without outliers, we do not have multicollinearity in data in the above summary table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1338305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso\n",
    "reg_lasso.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48740eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge\n",
    "reg_ridge.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639fda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbab8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm\n",
    "svm_regr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973c1d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = GradientBoostingRegressor(random_state=42)\n",
    "gbm.fit(X_train, y_train)\n",
    "GradientBoostingRegressor(random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3975969",
   "metadata": {},
   "source": [
    "7 Models new evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835ca536",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cross_val(models,X_validation=X_test, y_validation=y_test, cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613040f",
   "metadata": {},
   "source": [
    "== Regression application : conclusion  === \n",
    "* The models performance is better without outliers.  \n",
    "* Linear models are better than non-linear ones in general, but the difference is not so big than that. \n",
    "* The candidate models could be  gbm, Lasso and ridge with CV, i.e. Regularized linear regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15d8760",
   "metadata": {},
   "source": [
    "==== model evaluation : hyper parmeters search ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b16dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    " # IMPORT validation curve \n",
    "from sklearn.model_selection import validation_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26c687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbm hyper params search : gbm hyper-params \n",
    "gbm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search strategy : halving random search\n",
    "#from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "#from scipy.stats import uniform\n",
    "\n",
    "# hyper-params search\n",
    "distributions = dict(alpha=[0.1,0.2,0.3,0.4,0.5],\n",
    "                      learning_rate=[0.001,0.1,1,10],\n",
    "                      max_depth = [1,2,3,4])\n",
    "\n",
    "gbm_hp = RandomizedSearchCV(gbm, distributions,\n",
    "                            scoring = [\"max_error\",\"r2\"],\n",
    "                            random_state=0,\n",
    "                           refit = \"max_error\")\n",
    "search_gbm = gbm_hp.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fcedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_gbm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb5d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gbm evaluation\n",
    "gbm_hp2 = search_gbm.best_estimator_\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d07063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso hyper-params  list\n",
    "reg_lasso.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce23f39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lasso evaluation\n",
    "distributions = dict(alpha=[0.1,0.2,0.3,0.4,0.5])\n",
    "\n",
    "# hyper-params search \n",
    "lasso_hp = RandomizedSearchCV(reg_lasso, \n",
    "                              distributions, \n",
    "                              scoring = [\"max_error\",\"r2\"],\n",
    "                              refit = \"max_error\",\n",
    "                              random_state=0)\n",
    "search_lasso = lasso_hp.fit(X_train, y_train)\n",
    "lasso_hp2 = search_lasso.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae047d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best lasso estimator \n",
    "lasso_hp2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d69f62c",
   "metadata": {},
   "source": [
    "This lasso model/estimator  alpha is not different from the initial one. we hypothesize no amelioration in performance with respect to gbm estimator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ee09ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model evaluation\n",
    "# models dictionary\n",
    "models_hp = {\"gbm\" : gbm_hp2, \n",
    "             \"lasso\" :lasso_hp2}\n",
    "\n",
    "# models evaluation \n",
    "model_cross_val(models_hp,train0,y_train0,cv = 10)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cbdf10d",
   "metadata": {},
   "source": [
    "Results :\n",
    "We evaluated our models based on rmse. \n",
    "Gbm initial rmse was 0.08 and lasso 0.85. After CV in model evaluation, we choose gbm and lasso. In validation step , they present a rmse , 0.021 and 0.34 respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215cba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model validation : use test set\n",
    "model_cross_val({\"gbm\":gbm_hp2},test,y_test,cv = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3184af7c",
   "metadata": {},
   "source": [
    "GBM presents a rmse of 0.028 with test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822a8c09",
   "metadata": {},
   "source": [
    "Conclusion :  \n",
    "Following, hyper-parameters search, gbm shows a far better rmse of 0.021 and  lasso a rmse of 0.28.\n",
    "GBM performance improved with hyper-parameters search, while lasso did not. Therfore for tis application and in aregression setting, we would recommend gbm as final model in regression application. \n",
    "\n",
    "In test step (model evaluation)  gbm yields 0.028, wich is almost similar to the validation rmse. Howevr, we would recommend further , hyper-parameters serach and investigation to fine tune this model again with a large grid or grid strategy instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633ec29",
   "metadata": {},
   "source": [
    "Suggestion :\n",
    "We would suggest to further perform more hyper-params search to control any overfitting issue with gbm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4a42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocal message \n",
    "display(Javascript(\"\"\"\n",
    "  var msg = new SpeechSynthesisUtterance();\n",
    "  msg.text = \"End of file...\";\n",
    "  window.speechSynthesis.speak(msg);\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f400fd3917260b4df2a70e64dd1b6b0f213226eb51aa2d986fb9f86518ca676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
